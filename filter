
from __future__ import division
import math, sys, getopt, re, pprint, os, os.path, string, random, operator
from HTMLParser import HTMLParser
from nltk.tokenize import word_tokenize, wordpunct_tokenize, sent_tokenize
from nltk.stem.lancaster import LancasterStemmer
from nltk.corpus import stopwords
from nltk.tokenize.punkt import PunktWordTokenizer
import nltk
global true_positive, false_positive, true_negative, false_negative
true_positive = 0
false_positive = 0
true_negative = 0
false_negative = 0

def html_strip(line):
  return nltk.clean_html(line)

def tokenize(line):
	return PunktWordTokenizer().tokenize(line)

def fifteen_words(wordlist):
	fifteen={}
	for word in wordlist:
		fifteen[word]=abs(float(wordlist[word]-0.5))
	return dict(sorted(fifteen.iteritems(), key=operator.itemgetter(1), reverse=True)[:15])

def finalize(fifteen,sp):
	probs=[]
	for word in fifteen:
		probs.append(sp[word])
	product=1
	for p in probs:
		product *= p
	inv = [1-p for p in probs]
	inverse=1
	for i in inv:
		inverse *= i
	prob= float(product)/float(product+inverse)
	if prob>0.99:
		prob = 0.99
	if prob < 0.01:
		prob = 0.01
	return prob

def hammicity(wordlist,bad,good,nbad, ngood):
	spam_prob={}
	ham_prob={}
	hammicity={}
	for word in wordlist:
		if word not in good.keys():
			good[word] = 0.4
		if word not in bad.keys():
			bad[word]=0.4

	for word in wordlist:
		p=float(good[word])/float(ngood)
		if p > 1:
			p = 1
		ham_prob[word] = p
	for word in wordlist:
		p=float(bad[word])/float(nbad)
		if p >1:
			p = 1
		spam_prob[word] = p
	for word in wordlist:
		prob=float(ham_prob[word])/float(spam_prob[word]+ham_prob[word])
		if prob == 1.0:
			prob = 0.99
		hammicity[word]=prob
	return hammicity

def spamicity(wordlist,bad,good,nbad, ngood):
	spam_prob={}
	ham_prob={}
	spamicity={}
	for word in wordlist:
		if word not in good.keys():
			good[word] = 0.4
		if word not in bad.keys():
			bad[word]=0.4

	for word in wordlist:
		p=float(bad[word])/float(nbad)
		if p > 1:
			p = 1
		spam_prob[word] = p
	for word in wordlist:
		p=float(good[word])/float(ngood)
		if p >1:
			p = 1
		ham_prob[word] = p
	for word in wordlist:
		if good[word]+bad[word] > 5:
			prob=float(spam_prob[word])/float(spam_prob[word]+ham_prob[word])
			if prob == 1.0:
				prob = 0.99
			spamicity[word]=prob
		else:
			spamicity[word] = 0.4
	return spamicity

def ham_test(hamtest,good, bad, ngood, nbad):
	global true_positive,false_positive,true_negative,false_negative
	hamcount = 0
	n = 0
	wordprob={}
	for f in [f for f in os.listdir(hamtest) if os.path.isfile(os.path.join(hamtest,f))]:
		p = 1
		n += 1
		wordcount = filewords(hamtest+'/'+f)
		hp=hammicity(wordcount,bad,good,nbad,ngood)
		fifteen = fifteen_words(hp)
		hamprob = finalize(fifteen,hp)
		if hamprob > 0.9:
			hamcount+=1
			true_positive+=1
			true_negative+=1
		else:
			false_negative+=1
			false_positive+=1
	correct_classification = 100*(float(hamcount)/float(n))
	print "Size of ham testing set: " + str(n)
	print "Percentage of ham classified correctly: " + str(correct_classification)


def spam_test(spamtest,good, bad, ngood, nbad):
	global true_positive,false_positive,true_negative,false_negative
	spamcount = 0
	n = 0
	wordprob={}
	for f in [f for f in os.listdir(spamtest) if os.path.isfile(os.path.join(spamtest,f))]:
		p = 1
		n += 1
		wordcount = filewords(spamtest+'/'+f)
		sp=spamicity(wordcount,bad,good,nbad,ngood)
		fifteen = fifteen_words(sp)
		spamprob = finalize(fifteen,sp)
		if spamprob > 0.9:
			spamcount+=1
			true_positive+=1
			true_negative+=1
		else:
			false_negative+=1
			false_positive+=1			
	correct_classification = 100*(float(spamcount)/float(n))
	print "Size of spam testing set: " + str(n)
	print "Percentage of spam classified correctly: " + str(correct_classification)

def filewords(file):
	st = LancasterStemmer()
	doc=[]
	word_list={}
	fp = open(file,'r')
	for line in fp.readlines():
		line=html_strip(line)
		splitter=re.compile('\\W*')
		wordlist=[(st.stem(word)).lower() for word in tokenize(line)]## if word.isalpha() and len(word)>2 and len(word)<20]
		for word in wordlist:
			word_list[word] = 1
	return word_list

def words(dir):
	st = LancasterStemmer()
	files = [ f for f in os.listdir(dir) if os.path.isfile(os.path.join(dir,f)) ]
	doc=[]
	wordcount={}
	n=0
	for f in files:
		n+=1
		fp = open(dir+'/'+f,'r')
		for line in fp.readlines():
			line=html_strip(line)
			splitter=re.compile('\\W*')
			wordlist=[(st.stem(word)).lower() for word in tokenize(line)]## if word.isalpha() and len(word)>2 and len(word)<20]
			for word in wordlist:
				doc.append(word)
		word = dict ([(w,1) for w in doc])
		for w in word:
			if w in wordcount:
				wordcount[w] += 1
			else:
				wordcount[w] = 1
	return (wordcount,n)


class classifier:

	def __init__(self,hamtrain,spamtrain,hamtest,spamtest):
		self.hamtrain=hamtrain
		self.spamtrain=spamtrain
		self.spamtest=spamtest
		self.hamtest=hamtest

	def train(self):
		self.good,self.ngood=words(self.hamtrain)
		self.bad,self.nbad=words(self.spamtrain)
		print "Size of ham training set: " + str(self.ngood)
		print "Size of spam training set: " + str(self.nbad)
		self.n=self.nbad+self.ngood

	def test(self):
		ham_test(self.hamtest,self.good,self.bad,self.ngood,self.nbad)
		spam_test(self.spamtest,self.good,self.bad,self.ngood,self.nbad)
		
		
if __name__ == '__main__':
   optlist,args = getopt.getopt(sys.argv[1:],'hshs:',["hamtrain=","spamtrain=","hamtest=","spamtest="])
   hamtrain,spamtrain,hamtest,spamtest=optlist[0][1],optlist[1][1],optlist[2][1],optlist[3][1]
   c=classifier(hamtrain,spamtrain,hamtest,spamtest)
   c.train()
   c.test()
   accuracy=float(true_positive+true_negative)/float(true_positive+true_negative+false_negative+false_positive)
   accuracy*=100
   false_positive/=c.n
   false_positive*=100
   print "Total accuracy: " + str(accuracy)
   print "False Positives: " + str(false_positive)



